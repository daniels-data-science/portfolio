{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44fd5a6-3d46-41ee-a74e-29132d6458f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Transformer-based English to Russian AI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c188065f-2a60-45ad-9f0f-e62487c04278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\METABOX\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:37: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.6)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0 [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, optimizers\n",
    "print(\n",
    "    tf.__version__, \n",
    "    tf.config.list_physical_devices('GPU'), # GPU required for this task\n",
    "    keras.__version__\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd57255-ba9f-4183-be39-108f6b27f901",
   "metadata": {},
   "source": [
    "We will be using a text file provided by [Anki's Russian-English biligual sentence pairs](http://www.manythings.org/anki/) for this project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0796632-9ae8-4997-879d-cb42bfdf119d",
   "metadata": {},
   "source": [
    "## **Data Preprocessings**\n",
    "#### 1. Add `\"[start]\"` and `\"[end]\"` tokens to the Russian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e20447-ca23-49c2-af83-dd33a0d01ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_file_dir = \"rus.txt\"\n",
    "# this is a csv file containing English in the first column and Russian in the second\n",
    "\n",
    "with open(bilingual_file_dir, encoding='utf-8') as f: \n",
    "    lines = f.read().split(\"\\n\")\n",
    "text_pairs = []        \n",
    "for line in lines:\n",
    "    english, russian, _ = line.split(\"\\t\") # ignore the third column\n",
    "    russian = \"[start] \" + russian + \" [end]\"\n",
    "    text_pairs.append((english, russian))\n",
    "    \n",
    "print(len(text_pairs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf05867-7b8a-4346-8b69-3d0fa6399286",
   "metadata": {},
   "source": [
    "#### 2. Split the paired data into train/val/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0952c28b-d888-4138-815e-ae09d26c085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487600\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(len(train_pairs), len(val_pairs), len(test_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a640d3-b57b-4e5c-8edd-0d56761218c0",
   "metadata": {},
   "source": [
    "#### 3. Standardise sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6afd986-b5a6-4a2c-bee0-f71a71b72968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(string):\n",
    "    strip_chars = '\"#$%&\\'()*+,-./:;<=>@\\\\^_`{|}~—'\n",
    "    # English and Russian largely share the same punctuation\n",
    "    \n",
    "    string = tf.strings.lower(string)\n",
    "    \n",
    "    string = tf.strings.regex_replace(string, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "    # Remove characters in strip_chars using regular expression\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78ebd6-f575-4c32-baf5-ec0b9ca5d297",
   "metadata": {},
   "source": [
    "#### 4. Set tokenisation parameters\n",
    "`vocab_size`: the top *n* most frequent tokens in the training texts for both languages.\n",
    "`sequence_length`: English and Russian sentences cannot exceed this many words. Words after this cap will be cut off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d36335-152d-40bd-8872-995a7829c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 25_000\n",
    "sequence_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e711a46-1b38-426f-bfd5-50ab280a7fec",
   "metadata": {},
   "source": [
    "#### 5. Index each word to build a vocabulary\n",
    "`TextVectorization` on an example corpus:\n",
    "* \"I saw the red car.\"\n",
    "* \"The dog will bark at the car!\"\n",
    "\n",
    "After **standardisation**:\n",
    "  * \"i saw the red car\"\n",
    "  * \"the dog will bark at the car\"\n",
    "\n",
    "After **tokenisation**:\n",
    "  * [\"i\", \"saw\", \"the\", \"red\", \"car\"]\n",
    "  * [\"the\", \"dog\", \"will\", \"bark\", \"at\", \"the\", \"car\"]\n",
    "\n",
    "Each token is **replaced with a unique integer** ranging up to `vocab_size`, where '1' is the most frequent token in the corpus:\n",
    "  * [3, 4, 1, 5, 2]\n",
    "  * [1, 6, 7, 8, 9, 1, 2]\n",
    "\n",
    " \n",
    "We will be doing this same process for Anki's 487,600 pairs of Russian-English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf44584-14a7-4bff-87a6-b18bce4e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorization = layers.TextVectorization( # maps text to integer sequences\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=standardize\n",
    ")\n",
    "rus_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1, # needs to be one step ahead\n",
    "    standardize=standardize\n",
    ")\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_russian_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "# the .adapt() method builds the vocabulary from input data\n",
    "eng_vectorization.adapt(train_english_texts)\n",
    "rus_vectorization.adapt(train_russian_texts)\n",
    "\n",
    "rus_index_lookup = dict(enumerate(rus_vectorization.get_vocabulary()))\n",
    "# {0: '', 1: '[UNK]', 2: '[start]', 3: '[end]', 4: 'Я', 5: 'не',... `vocab_size`-1: 'намереваешься'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be256b3-20fd-4fcc-8fd8-22d0dfd0934e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "\n",
    "def format_dataset(eng, rus):\n",
    "    eng = eng_vectorization(eng)\n",
    "    rus = rus_vectorization(rus)\n",
    "    return (\n",
    "        {\"english\": eng, \"russian\": rus[:, :-1]}, \n",
    "        # these dict keys will also be the model's Input layers' names\n",
    "        rus[:, 1:]\n",
    "    )\n",
    "    # returns a tuple that will be the model's two inputs\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, rus_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    rus_texts = list(rus_texts)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, rus_texts))\n",
    "    # # Slicing a tuple of 1D tensors produces tuple elements containing scalar tensors.\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=8)\n",
    "    \n",
    "    dataset = dataset.shuffle(2048).prefetch(16).cache()\n",
    "    # uses in-memory caching to speed up preprocessing.\n",
    "    return dataset\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "test_ds = make_dataset(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a5ccdd-3ce4-48d9-9969-7c7980d369fc",
   "metadata": {},
   "source": [
    "## **Architecture**\n",
    "\n",
    "The transformer will the architecture as proposed in [Vaswani et al.'s \"*Attention Is All You Need*\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "![Vaswini et. al](https://www.researchgate.net/publication/339390384/figure/fig1/AS:860759328321536@1582232424168/The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0206ffe4-e8d4-452f-a112-7f7348133d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256 # each embedded token is a vector of this length. \n",
    "#For ChatGPT, this value would be about 12,000\n",
    "\n",
    "dense_dim = 1024\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838eefa-95ff-4b43-875f-dc2678126768",
   "metadata": {},
   "source": [
    "The `PositionalEmbedding` combines *two* different embeddings: \n",
    "* token embeddings, which regard each word's unique semantics\n",
    "* position embeddings, which provide a unique embedding for each position in the sequence up to `sequence_length`. It ensures that the model can account for the order of tokens within the sequence.\n",
    "\n",
    "Adding together those two embeddings integrates their information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9ffe17-469e-4c8c-988c-ddb4dfdefdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # since sentences vary in length, shorter sentences will be padded\n",
    "        # masking is to avoid considering padded positions\n",
    "        \n",
    "        return tf.math.not_equal(inputs, 0) # does not work on latest tf\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        \n",
    "        embedded_tokens = self.token_embeddings(inputs) \n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "\n",
    "        embeddeding = embedded_tokens + embedded_positions\n",
    "        # combines position information with token representation\n",
    "        \n",
    "        return embeddeding  \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'output_dim':self.output_dim,\n",
    "            'sequence_length':self.sequence_length,\n",
    "            'input_dim':self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98616bae-516b-4f5d-9ca6-3a3a11efc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim # size of token vectors, also the key_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(\n",
    "        # for every attention head:\n",
    "            # Q, K and V all have independent dense connections\n",
    "            # Then, the dense projection's are inputted into attention\n",
    "        # attention outputs of each head are concatenated together\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        \n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.multi_head_attention(\n",
    "            inputs, inputs, attention_mask=mask\n",
    "        )\n",
    "        before_fc = self.layernorm_1(inputs + attention_output)\n",
    "        after_fc  = self.dense_proj(before_fc)\n",
    "\n",
    "        final_output = self.layernorm_2(before_fc + after_fc)\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config  \n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.supports_masking = True\n",
    "        \n",
    "        self.multi_head_attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        \n",
    "        self.multi_head_attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "        self.dense_proj  = keras.Sequential([\n",
    "            layers.Dense(self.dense_dim, activation='relu'),\n",
    "            layers.Dense(self.embed_dim)\n",
    "        ])\n",
    "\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs): # necessary to prevent information leakage from future timesteps\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        \n",
    "        mask = tf.linalg.band_part( # returns lower triangle\n",
    "            tf.ones((sequence_length,sequence_length)), -1, 0\n",
    "        )\n",
    "        mask = tf.cast(mask, 'int32')\n",
    "        \n",
    "        mask = tf.tile( # Replicate 100 times along the new axis\n",
    "            mask[tf.newaxis, :, :], \n",
    "            \n",
    "            [batch_size, 1, 1] \n",
    "        )\n",
    "        return mask \n",
    "        # mask has shape: (batch_size, sequence_length, sequence_length)\n",
    "        \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:,tf.newaxis,:], dtype='int32'\n",
    "            )\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "            \n",
    "        multi_head_1_output = self.multi_head_attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, \n",
    "            attention_mask=causal_mask\n",
    "        )\n",
    "\n",
    "        layernorm_1_output = self.layernorm_1(\n",
    "            inputs + multi_head_1_output\n",
    "        )\n",
    "\n",
    "        multi_head_attention_2_output = self.multi_head_attention_2(\n",
    "            query=multi_head_1_output, value=encoder_outputs, key=encoder_outputs, \n",
    "            attention_mask=padding_mask\n",
    "        )\n",
    "\n",
    "        layernorm_2_output = self.layernorm_2(\n",
    "            layernorm_1_output + multi_head_attention_2_output\n",
    "        )\n",
    "\n",
    "        dense_proj_output = self.dense_proj(\n",
    "            layernorm_2_output\n",
    "        )\n",
    "\n",
    "        layernorm_3_output = self.layernorm_3(\n",
    "            layernorm_2_output + dense_proj_output\n",
    "        )\n",
    "        return layernorm_3_output\n",
    "             \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884aa00-2df9-4f87-8254-cf031a8e7614",
   "metadata": {},
   "source": [
    "### Source sequence (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c338601e-b8a7-4290-8b2f-ddb9d8e38c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_input_layer = keras.Input(\n",
    "    shape=(None,), # transformers are shape invariant\n",
    "    dtype=\"int64\", \n",
    "    name=\"english\"\n",
    ")\n",
    "\n",
    "english_embedding = PositionalEmbedding(\n",
    "    sequence_length = sequence_length,\n",
    "    input_dim = vocab_size, \n",
    "    output_dim = embed_dim, \n",
    "    name = 'english_embedding'\n",
    ")(english_input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b4a85-b8b1-47e7-b625-361dbc36205b",
   "metadata": {},
   "source": [
    "### Target sequence (russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed90b8cd-4393-4969-8878-13c46c48d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_input_layer = keras.Input(\n",
    "    shape=(None,), # transformers are shape invariant\n",
    "    dtype=\"int64\", \n",
    "    name = 'russian'\n",
    ")\n",
    "\n",
    "russian_embedding = PositionalEmbedding(\n",
    "    sequence_length = sequence_length,\n",
    "    input_dim = vocab_size, \n",
    "    output_dim = embed_dim, \n",
    "    name = 'russian_embedding'\n",
    ")(russian_input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b74c6-92a6-4983-a236-82476e70b19b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be99bd1-be03-4f54-be1e-8ee7694ffaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_english = TransformerEncoder(\n",
    "    embed_dim = embed_dim,\n",
    "    dense_dim = dense_dim,\n",
    "    num_heads = num_heads,\n",
    "    name = 'encoder'\n",
    ")(english_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda21e7c-4ed1-460b-b28f-5776007c296d",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88c3d1b-9c05-4984-886b-3f6cf5125036",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TransformerDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    dense_dim=dense_dim,\n",
    "    num_heads=num_heads\n",
    ")(russian_embedding, encoder_outputs=encoded_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f859a79-ae8c-44f8-bdaa-a561c33c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dropout(0.4)(x)\n",
    "output_layer = layers.Dense(vocab_size, activation='softmax', name='output_layer')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90615126-feb5-4ea2-becf-e0d0f773ffa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " russian (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " english_embedding (PositionalE  (None, None, 256)   6405120     ['english[0][0]']                \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " russian_embedding (PositionalE  (None, None, 256)   6405120     ['russian[0][0]']                \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " encoder (TransformerEncoder)   (None, None, 256)    2630144     ['english_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   4734208     ['russian_embedding[0][0]',      \n",
      " erDecoder)                                                       'encoder[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, None, 25000)  6425000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,599,592\n",
      "Trainable params: 26,599,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "seq2seq_transformer = keras.Model(\n",
    "    inputs = [english_input_layer, russian_input_layer],\n",
    "    outputs= output_layer\n",
    ")\n",
    "print(seq2seq_transformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ce5a43-7abe-46f4-b82b-a5e1d8f63308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(seq2seq_transformer, show_shapes=True, show_layer_names=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75758f9-e8a0-4332-b05d-ea28af823427",
   "metadata": {},
   "source": [
    "## **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f3dde36-ad80-45fd-bfb4-855d93514598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "2667/2667 [==============================] - 1057s 395ms/step - loss: 0.9090 - accuracy: 0.6003 - val_loss: 0.5379 - val_accuracy: 0.6997\n",
      "Epoch 2/64\n",
      "2667/2667 [==============================] - 1077s 404ms/step - loss: 0.5196 - accuracy: 0.7046 - val_loss: 0.4389 - val_accuracy: 0.7340\n",
      "Epoch 3/64\n",
      "2667/2667 [==============================] - 1080s 405ms/step - loss: 0.4192 - accuracy: 0.7391 - val_loss: 0.4059 - val_accuracy: 0.7480\n",
      "Epoch 4/64\n",
      "2667/2667 [==============================] - 1082s 406ms/step - loss: 0.3663 - accuracy: 0.7601 - val_loss: 0.3914 - val_accuracy: 0.7566\n",
      "Epoch 5/64\n",
      "2667/2667 [==============================] - 1078s 404ms/step - loss: 0.3314 - accuracy: 0.7758 - val_loss: 0.3865 - val_accuracy: 0.7618\n",
      "Epoch 6/64\n",
      "2667/2667 [==============================] - 1076s 403ms/step - loss: 0.3063 - accuracy: 0.7876 - val_loss: 0.3847 - val_accuracy: 0.7652\n",
      "Epoch 7/64\n",
      "2667/2667 [==============================] - 1073s 402ms/step - loss: 0.2865 - accuracy: 0.7973 - val_loss: 0.3840 - val_accuracy: 0.7669\n",
      "Epoch 8/64\n",
      "2667/2667 [==============================] - 1066s 400ms/step - loss: 0.2699 - accuracy: 0.8057 - val_loss: 0.3838 - val_accuracy: 0.7692\n",
      "Epoch 9/64\n",
      "2667/2667 [==============================] - 1090s 409ms/step - loss: 0.2567 - accuracy: 0.8128 - val_loss: 0.3851 - val_accuracy: 0.7701\n",
      "Epoch 10/64\n",
      "2667/2667 [==============================] - 1094s 410ms/step - loss: 0.2450 - accuracy: 0.8189 - val_loss: 0.3883 - val_accuracy: 0.7720\n",
      "Epoch 11/64\n",
      "2667/2667 [==============================] - 1087s 408ms/step - loss: 0.2349 - accuracy: 0.8243 - val_loss: 0.3927 - val_accuracy: 0.7721\n",
      "Epoch 12/64\n",
      "2667/2667 [==============================] - 1087s 408ms/step - loss: 0.2264 - accuracy: 0.8291 - val_loss: 0.3950 - val_accuracy: 0.7735\n",
      "Epoch 13/64\n",
      "2667/2667 [==============================] - 1085s 407ms/step - loss: 0.2188 - accuracy: 0.8335 - val_loss: 0.3931 - val_accuracy: 0.7742\n",
      "Epoch 14/64\n",
      "2667/2667 [==============================] - 1088s 408ms/step - loss: 0.2118 - accuracy: 0.8372 - val_loss: 0.3973 - val_accuracy: 0.7738\n",
      "Epoch 15/64\n",
      "2667/2667 [==============================] - 1071s 401ms/step - loss: 0.2065 - accuracy: 0.8404 - val_loss: 0.4011 - val_accuracy: 0.7749\n",
      "Epoch 16/64\n",
      "2667/2667 [==============================] - 1092s 409ms/step - loss: 0.2012 - accuracy: 0.8437 - val_loss: 0.4056 - val_accuracy: 0.7746\n",
      "Epoch 17/64\n",
      "2667/2667 [==============================] - 1118s 419ms/step - loss: 0.1968 - accuracy: 0.8459 - val_loss: 0.4042 - val_accuracy: 0.7754\n",
      "Epoch 18/64\n",
      "2667/2667 [==============================] - 1088s 408ms/step - loss: 0.1929 - accuracy: 0.8482 - val_loss: 0.4065 - val_accuracy: 0.7747\n",
      "Epoch 19/64\n",
      "2667/2667 [==============================] - 1122s 421ms/step - loss: 0.1890 - accuracy: 0.8506 - val_loss: 0.4121 - val_accuracy: 0.7755\n",
      "Epoch 20/64\n",
      "2667/2667 [==============================] - 1107s 415ms/step - loss: 0.1858 - accuracy: 0.8523 - val_loss: 0.4141 - val_accuracy: 0.7755\n",
      "Epoch 21/64\n",
      "2667/2667 [==============================] - 1080s 405ms/step - loss: 0.1831 - accuracy: 0.8540 - val_loss: 0.4150 - val_accuracy: 0.7756\n",
      "Epoch 22/64\n",
      "2667/2667 [==============================] - 1055s 395ms/step - loss: 0.1807 - accuracy: 0.8555 - val_loss: 0.4177 - val_accuracy: 0.7750\n",
      "Epoch 23/64\n",
      "2667/2667 [==============================] - 1048s 393ms/step - loss: 0.1781 - accuracy: 0.8572 - val_loss: 0.4167 - val_accuracy: 0.7755\n",
      "Epoch 24/64\n",
      "2667/2667 [==============================] - 42661s 16s/step - loss: 0.1761 - accuracy: 0.8583 - val_loss: 0.4194 - val_accuracy: 0.7757\n",
      "Epoch 25/64\n",
      "2667/2667 [==============================] - 1141s 428ms/step - loss: 0.1750 - accuracy: 0.8589 - val_loss: 0.4234 - val_accuracy: 0.7756\n",
      "Epoch 26/64\n",
      "2667/2667 [==============================] - 1578s 592ms/step - loss: 0.1735 - accuracy: 0.8599 - val_loss: 0.4250 - val_accuracy: 0.7742\n",
      "Epoch 27/64\n",
      "2667/2667 [==============================] - 1861s 698ms/step - loss: 0.1720 - accuracy: 0.8607 - val_loss: 0.4255 - val_accuracy: 0.7743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b379663d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "seq2seq_transformer.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"] # next token accuracy\n",
    ")\n",
    "\n",
    "seq2seq_transformer.fit(\n",
    "    train_ds, \n",
    "    epochs=64, # may be stopped by EarlyStopping before reaching this number\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc2a23c-32f0-4c31-8654-2c71dadedf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(seq2seq_transformer,'eng_rus_translate.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8db98ac0-1603-4333-b649-1865f6196d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 106s 185ms/step - loss: 0.4230 - accuracy: 0.7752\n",
      "{'loss': 0.42303743958473206, 'accuracy': 0.7751858830451965}\n"
     ]
    }
   ],
   "source": [
    "print(seq2seq_transformer.evaluate(test_ds, return_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bd431-f38e-43b3-aa5e-d1b9f6a30c1f",
   "metadata": {},
   "source": [
    "The 'accuracy' metric refers to predicting the next word in the sequence correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45d3b7-bb56-4859-ba47-68a8121c4f75",
   "metadata": {},
   "source": [
    "## **Manual Evaluation**\n",
    "Firstly, need a way to see the examples of the sentences being translated. Then, 50 sample will be manually cross-checked with Google Translate and judged for their accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61ebcc4-a3a6-4f8d-97f6-d3197a5c01bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom told Mary he needed her help.\n",
      "[start] Том сказал Мэри что ему нужна её помощь [end]\n",
      "\n",
      "Do you like opera?\n",
      "[start] Ты любишь оперу? [end]\n",
      "\n",
      "We adopted a baby.\n",
      "[start] Мы усыновили ребёнка [end]\n",
      "\n",
      "Could you talk a little slower?\n",
      "[start] Ты не могла бы говорить немного помедленнее? [end]\n",
      "\n",
      "I don't think I'll be able to eat all this.\n",
      "[start] Не думаю что смогу это всё съесть [end]\n",
      "\n",
      "The police are going to catch you sooner or later.\n",
      "[start] Полиция рано или поздно тебя поймает [end]\n",
      "\n",
      "Tom was aware that Mary was married.\n",
      "[start] Том был в курсе что Мэри замужем [end]\n",
      "\n",
      "Tom makes about fifty phone calls a day.\n",
      "[start] Том делает пять звонков телефона [end]\n",
      "\n",
      "I don't want to talk about it right now.\n",
      "[start] Я не хочу сейчас об этом говорить [end]\n",
      "\n",
      "Please don't do anything else.\n",
      "[start] Пожалуйста не делай ничего ещё [end]\n",
      "\n",
      "Tom wants to go with us.\n",
      "[start] Том хочет пойти с нами [end]\n",
      "\n",
      "I really don't know what to say.\n",
      "[start] Я правда не знаю что сказать [end]\n",
      "\n",
      "You need me.\n",
      "[start] Я вам нужен [end]\n",
      "\n",
      "Tom likes Mary.\n",
      "[start] Тому нравится Мэри [end]\n",
      "\n",
      "Everyone is scared.\n",
      "[start] Все напуганы [end]\n",
      "\n",
      "It's getting dark.\n",
      "[start] Темнеет [end]\n",
      "\n",
      "We made a huge mistake.\n",
      "[start] Мы совершили огромную ошибку [end]\n",
      "\n",
      "I work better alone than in a group.\n",
      "[start] Я лучше в одиночку скучаю по группе [end]\n",
      "\n",
      "Tom is an excellent cook.\n",
      "[start] Том отличный повар [end]\n",
      "\n",
      "Let's go away.\n",
      "[start] Давай уйдём [end]\n",
      "\n",
      "John took a key from his pocket.\n",
      "[start] Том достал из кармана ключ [end]\n",
      "\n",
      "You must study French.\n",
      "[start] Ты должен учить французский [end]\n",
      "\n",
      "Whenever someone knocks on the door, my dog starts barking.\n",
      "[start] Каждый когда ктонибудь стучит в дверь моя собака начинает лаять [end]\n",
      "\n",
      "There's time.\n",
      "[start] Время есть [end]\n",
      "\n",
      "I don't think you'll have the same problems I had.\n",
      "[start] Не думаю что у тебя будут думать те проблемы что у меня были [end]\n",
      "\n",
      "How many times have you done this?\n",
      "[start] Сколько раз ты это сделал? [end]\n",
      "\n",
      "You may use mine if you want.\n",
      "[start] Можешь взять мой если хочешь [end]\n",
      "\n",
      "She is wearing a white dress today.\n",
      "[start] Она сегодня в белом платье [end]\n",
      "\n",
      "Don't say bad things about others.\n",
      "[start] Не говори о других плохо [end]\n",
      "\n",
      "Who gave you permission to do that?\n",
      "[start] Кто дал тебе разрешение сделать это? [end]\n",
      "\n",
      "I'm not suggesting you go alone.\n",
      "[start] Я не предлагаю тебе ехать одному [end]\n",
      "\n",
      "Why does Tom need a hammer?\n",
      "[start] Зачем Тому молоток? [end]\n",
      "\n",
      "Have you ever been to Japan?\n",
      "[start] Ты когданибудь был в Японии? [end]\n",
      "\n",
      "I had to go there.\n",
      "[start] Мне надо было туда пойти [end]\n",
      "\n",
      "His mother was a singer.\n",
      "[start] Его мать была певицей [end]\n",
      "\n",
      "It is ten degrees below zero now.\n",
      "[start] Сейчас десять градусов ниже нуля [end]\n",
      "\n",
      "I locked myself out.\n",
      "[start] Я запер дверь оставив ключи внутри [end]\n",
      "\n",
      "I couldn't sleep either.\n",
      "[start] Я тоже не мог уснуть [end]\n",
      "\n",
      "We're both students.\n",
      "[start] Мы оба студенты [end]\n",
      "\n",
      "Why did you cry?\n",
      "[start] Почему ты плакал? [end]\n",
      "\n",
      "Tom certainly wasn't ready for it.\n",
      "[start] Том определённо не был к этому готов [end]\n",
      "\n",
      "I can answer your questions.\n",
      "[start] Я могу ответить на твои вопросы [end]\n",
      "\n",
      "He thinks that I am in love with her.\n",
      "[start] Он думает что я люблю её [end]\n",
      "\n",
      "Let me think about it.\n",
      "[start] Дайте мне об этом подумать [end]\n",
      "\n",
      "Didn't you know Tom had a house on Park Street?\n",
      "[start] Ты не знал что у Тома дом на Парковой улице? [end]\n",
      "\n",
      "Tom is a judge.\n",
      "[start] Том Судья [end]\n",
      "\n",
      "Tom told me the whole story.\n",
      "[start] Том рассказал мне всю историю [end]\n",
      "\n",
      "I made up my mind to go there.\n",
      "[start] Я решил пойти туда [end]\n",
      "\n",
      "Tom has no authority to do that.\n",
      "[start] У Тома нет полномочий чтобы это сделать [end]\n",
      "\n",
      "Tom doesn't use Facebook.\n",
      "[start] Том не пользуется Фейсбуком [end]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(\n",
    "    input_sentence, \n",
    "    max_decoded_sentence_length = sequence_length\n",
    "):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = rus_vectorization([decoded_sentence])\n",
    "        \n",
    "        predicted_next_token = seq2seq_transformer.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence[:,:-1]], \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        sampled_token_index = np.argmax(predicted_next_token[0, i, :])\n",
    "        sampled_token = rus_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "random.seed(0)\n",
    "with open(\"translation_output.txt\", \"w\") as file:\n",
    "    for _ in range(50):\n",
    "        decoded_sentence = \"[UNK]\"\n",
    "        # continue fetching and decoding until a sentence without [UNK] is decoded\n",
    "        while \"[UNK]\" in decoded_sentence:\n",
    "            random_english_sentence_index = random.randint(0, num_val_samples - 1)\n",
    "            input_sentence = test_pairs[random_english_sentence_index][0]\n",
    "            decoded_sentence = decode_sequence(input_sentence)\n",
    "        file.write(f\"Original: {input_sentence}\\n\")\n",
    "        file.write(f\"Decoded: {decoded_sentence}\\n\\n\")\n",
    "        print(input_sentence)\n",
    "        print(decoded_sentence)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545045a-0afb-4a94-ad9f-c4e66677e62d",
   "metadata": {},
   "source": [
    "#### 48* of 50 correct!\n",
    "_Translations with minor grammatical errors counted as correct if the meaning was preserved._\n",
    "\n",
    "The two incorrectly translated sentences were:\n",
    "* \"John took a key from his pocket\" to **\"Том достал из кармана ключ\"**\n",
    "    * \"John\" was incorrectly translated as \"Tom,\" because \"Tom\" is a frequently used name in the training data.\n",
    "* \"Tom makes about fifty phone calls a day.\" to **\"Том делает пять звонков телефона\"**\n",
    "    * Missing \"about\" and \"a day,\" and used the wrong case for \"телефона.\"\n",
    " \n",
    "Minor errors in tense, gender, or case also occurred, probably due to the training data not being comprehensive enough to capture contextual nuances of grammatical forms.\n",
    "\n",
    "To enhance translation quality in future similar projects, it's crucial to use a more diverse training dataset covering various language styles and contexts. Additionally, increasing computational capabilities, such as expanding embedding dimensions and adding more attention heads, will help the model better manage complex language structures. These upgrades will improve the model's grammatical accuracy and overall translation quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
